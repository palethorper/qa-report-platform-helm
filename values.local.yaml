
# https://github.com/elastic/helm-charts/blob/main/logstash/values.yaml
#TODO: Add opensearch output plugin!!

global:
  hostname: &fqdn "devlocal.groundzero.solutions"
  loadBalancerHostname: &do-lb-workaround-hostname "devlocal-lb.groundzero.solutions"

  ospassword: &ospassword "Test@Password01"
  msgbrokerPassword: &msgbrokerPassword P@ssw0rd11!!

  ip-whitelist-internal: &ipwhiteliststring "0.0.0.0/0"
  
  ingressClassName: &ingressClassName "devlocal-dev-nginx"

  certificateIssuerName: &certificateIssuerName "letsencrypt-prod"
  

  # letsencrypt-contact: "rpalethorpe@groundzero.solutions" # TODO: Fix contact paramter [Low]

ingress-nginx:
  install: true

  tcp:
    "5044": "devlocal/qai1-logstash:5044"
    "50000": "devlocal/qai1-logstash:50000"
    "9092": "devlocal/qai-kafka:9092"
    # "9200": "devlocal/qai-elasticsearch:9200"

  controller:
    progressDeadlineSeconds: 600
    # ingressClass: test-nginx
    ingressClassResource:
      name: *ingressClassName
    config:
      allow-snippet-annotations: "true"
      # use-proxy-protocol: "true"
    publishService:
      enabled: true
    service:
    #   annotations:
        # service.beta.kubernetes.io/do-loadbalancer-hostname: *do-lb-workaround-hostname
        # service.beta.kubernetes.io/do-loadbalancer-name: *do-lb-workaround-hostname
        # service.beta.kubernetes.io/do-loadbalancer-size-unit: "1"
        # service.beta.kubernetes.io/do-loadbalancer-type: "REGIONAL_NETWORK"
        # service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: "true"
        # service.beta.kubernetes.io/do-loadbalancer-deny-rules: "cidr:198.51.100.0/16"
      loadBalancerSourceRanges:
        - *ipwhiteliststring
        # - 0.0.0.0/0

logstash:
  install: true

  # image: logstash-opensearch
  image: "registry.digitalocean.com/g0-cr/logstash-opensearch"
  imageTag: latest
  imagePullSecrets:
    - name: g0-cr

  persistence:
    enabled: false

  logstashConfig:
    logstash.yml: |
      api.http.host: "0.0.0.0"
      # xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
      # xpack.monitoring.enabled: false

      
  envFrom:
    - secretRef:
        name: platform-creds

  logstashPipeline:
    logstash.conf: |
      # override default, hardcoded, file in template.
    01_input_default.conf: |
      input {
        beats {
          port => 5044
        }
        http {
          port => 50000
          response_code => 200
          ssl_enabled => false
        }
      }
    
    01_input_file-tosca-xml.conf: |
      input {
        # file {
        # 	# tags => [ "debug" ] 

        # 	path => "/usr/share/logstash/files/export*.xml"

        # 	# TODO:
        # 	# If is an EL entry - don't take
        # 	codec => multiline {
        # 		pattern => '^\s{2}\<\w+\>\w*$'
        # 		# pattern => "<(TestCase|ExecutionList|ExecutionLogs|TestCaseLog|ActualLog)>" # AllEntries|All_TestCaseLogs|All_ExecutionLogs
        # 		negate => "true"
        # 		what => "previous"
        # 		max_lines => 20000
        # 	}

        # 	type => "xml"
        # 	start_position => "beginning"
          # }

        # http_poller {
        # 	add_field => { 
        # 		"product" => "healthcheck"
        # 		"ccode" => "csr"
        # 		"country_code" => "au"
        # 	}
        # 	urls => {
        # 		consul_health_state => "http://${CONSUL_HEALTH_URL:localhost}:8500/v1/health/state/any"
        # 	}

        # 	codec => "json"
        # 	schedule => { every => "60s" }
        # }
      }
    # 02_filter_http-split: |
    #   filter {
    #     split{
    #       field => "[event][original]"
    #     }
    #   }

    02_filter_tosca.conf: |
      filter {
        if [Product] =~ "Tosca.*" and ![AddedToListDate] {

          # Get rid of the header message in xml
          if [message] =~ /xml version=/ { drop { } }


          # xml {
          # 	source => "message"
          # 	store_xml => true
          # 	target => "_xml"
          # 	force_array => false
          # 	# max_lines => 5000
          # }

          mutate {
            add_field => { "[@metadata][index_prefix]" => "%{Product}-%{CustCode}" }
            lowercase => [ "[@metadata][index_prefix]" ]
            # add_field => { "[index_prefix]" => "%{[@metadata][index_prefix]}" }
            add_field => { "eventtime"=> "%{@timestamp}" }
          }

          ruby {
            code => '
              tags = [] if tags.nil?

              x = event.get("[@metadata][index_prefix]").downcase
              event.set("[@metadata][index_prefix]",x)

              y = event.get("_xml")

              if !y.nil?
                y.each {|k, v|
                  v.gsub!(/^true$/i,"true") if !v.nil?
                  v.gsub!(/^false$/i,"false") if !v.nil?
                  event.set(k,v)
                }
              end

              # Keep metadata
              if tags.include?("debug") then
                event.get("[@metadata]").each {|k,v| event.set("[meta][#{k}]", v)}
              end

              event.remove("_xml")
              # event.remove("message")
            '
          }

          # TODO: What if the Modified Date is salso Unknown?
          
          # Workaround for value of 'unknown'
          if [CreatedAt] == "Unknown" {
            mutate {
              remove_field => [ "[CreatedAt]" ]
              add_tag => [ "date-unknown", "CreatedAt" ]
            }
          }
          
          if [ModifiedAt] == "Unknown" {
            mutate {
              remove_field => [ "[ModifiedAt]" ]
            }

            # Only add if not already added ..
            if "date-unknown" not in [tags] {
              mutate {
                add_tag => [ "date-unknown", "ModifiedAt" ]
              }
            }

          }

          date {
            # match => ["[ModifiedAt]","dd/MM/yyyy H:mm:ss a","dd/M/yyyy H:mm:ss a", "ISO8601", "yyyy-MM-dd HH:mm:ssZ"] # "MM/dd/yyyy H:mm:ss a","M/dd/yyyy HH:mm:ss a",
            match => ["[ModifiedAt]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            # timezone => "Australia/Sydney"
            target => "@timestamp"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            # match => ["[ModifiedAt]","dd/MM/yyyy H:mm:ss a","dd/M/yyyy H:mm:ss a", "ISO8601", "yyyy-MM-dd HH:mm:ssZ"] # "MM/dd/yyyy H:mm:ss a","M/dd/yyyy HH:mm:ss a",
            match => ["[ModifiedAt]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            # timezone => "Australia/Sydney"
            target => "ModifiedAt"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            # match => ["[ModifiedAtDate]","dd/MM/yyyy H:mm:ss a","dd/M/yyyy H:mm:ss a", "ISO8601", "yyyy-MM-dd H:mm:ssZ"] # "MM/dd/yyyy H:mm:ss a","M/dd/yyyy H:mm:ss a",
            match => ["[ModifiedAtDate]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            # timezone => "Australia/Sydney"
            target => "ModifiedAtDate"
            tag_on_failure => ["_dateparsefailure_ModifiedAtDate"]
          }

          date {
            # match => ["[CreatedAt]","dd/MM/yyyy H:mm:ss a","dd/M/yyyy H:mm:ss a", "ISO8601", "yyyy-MM-dd HH:mm:ssZ"] # "MM/dd/yyyy H:mm:ss a","M/dd/yyyy H:mm:ss a",
            match => ["[CreatedAt]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]

            # timezone => "Australia/Sydney"
            target => "CreatedAt"
            tag_on_failure => ["_dateparsefailure_CreatedAt"]
          }

          date {
            match => ["[CreatedAtDate]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            target => "CreatedAtDate"
            tag_on_failure => ["_dateparsefailure_CreatedAtDate"]
          }

          # Set Execution list evetime time to StartTime
          date {
            match => ["[StartTime]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            target => "@timestamp"
            tag_on_failure => ["_dateparsefailure_StartTime"]
          }

          date {
            match => ["[StartTime]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            target => "StartTime"
            tag_on_failure => ["_dateparsefailure_StartTime"]
          }

          if [ObjectType] == "ExecutionTestCaseLog" and [StartTime] {
            mutate {
              update => { "StartTime" => "%{@timestamp}" }
            }
          }

          date {
            # match => ["[EndTime]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            match => ["[EndTime]","ISO8601", "yyyy-MM-dd HH:mm:ssZ"]
            # timezone => "Australia/Sydney"
            target => "EndTime"
            tag_on_failure => ["_dateparsefailure_EndTime"]
          }


          mutate {
            gsub => [ "NodePath", "^\s*", "", "message", "\s*$", ""]

            convert => { "UpdateRevision" => "integer" }
            convert => { "Revision" => "integer" }

            convert => { "HasMissingReferences" => "boolean" }
            convert => { "IsTemplate" => "boolean" }
            convert => { "Disabled" => "boolean" }
            convert => { "IncludeForSynchronization" => "boolean" }
            convert => { "IsBusinessTestCase" => "boolean" }
            convert => { "ChangesAllowed" => "boolean" }
            convert => { "IsCheckedOutByMe" => "boolean" }

            # ExecutionList
            convert => { "NumberOfTestCases" => "integer" }
            convert => { "NumberOfTestCasesFailed" => "integer" }
            convert => { "NumberOfTestCasesPassed" => "integer" }
            convert => { "NumberOfTestCasesNotExecuted" => "integer" }
            convert => { "NumberOfTestCasesWithUnknownState" => "integer" }

            convert => { "IsViewingAllowed" => "boolean" }
            convert => { "IsOsvItem" => "boolean" }
            convert => { "HasNotUniqueNamedSubparts" => "boolean" }
            convert => { "OwningGroupInherited" => "boolean" }
            convert => { "IncludeInAnalytics" => "boolean" }
            convert => { "IsMandate" => "boolean" }
            convert => { "HasMandates" => "boolean" }
            convert => { "ContainsClassic" => "boolean" }
            convert => { "IsBusinessExecutionList" => "boolean" }
            convert => { "HasInconsistentParallelizationState" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }

            convert => { "ManualItems" => "integer" }
            convert => { "AutomatedItems" => "integer" }

            #TestCaseLog
            convert => { "IsDeletedFromFileService" => "boolean" }
            convert => { "RetainOnFileService" => "boolean" }
            convert => { "Canceled" => "boolean" }
            convert => { "Recovered" => "boolean" }

            convert => { "Duration" => "float" }

            # TODO: Only remove original event if NOT failed processing
            remove_field => [ "[event][original]", "[$id]" ]
            # remove_field => [ "[log][file]][path]" ]
          }

          # Drop soem duplicates
          # if [ObjectType] == 'TestCase' and [UpdateRevision] == ''  { drop { } }

          kv {
            source => "TCProperties"
            target => "[Properties]"
            include_brackets => false
            field_split => ' '
            trim_key => " "
            remove_field => [ "TCProperties" ]
          }

          ruby  {
            code => '
              
              # begin 

              r = event.get("[NodePath]")
              if (r.to_s.scan("/").count > 0 && r !=nil )
                paths = r.split("/").reject {|s| s.empty? }

                t = paths.delete_if { |component| component.empty? }
                event.set("[ComponentFolders]", t)

                tags = event.get("[tags]")
                tags = [] if tags.nil?
                t.each{|x| tags << x }
                event.set("[tags]", tags)

                t.each {|x| 
                  event.set("[Folder]", x)
                } 
                
                folder = r[/^(.*)(\/.*?)$/, 1]
                event.set("[Folder]", folder)

              end

              # rescue
              # end
            '}

          ruby {
            code => '
              t = Time.at(event.get("@timestamp").to_f)
              suffix = t.strftime("%Y%m%d%H%M%S")
              event.set("[@metadata][id_suffix]", suffix)

              suffix = t.strftime("%Y-%m")
              # suffix = ("-" + suffix + "-%{[Workspace]}").downcase
              event.set("[@metadata][index_suffix]", "-" + suffix)
            '
          }

          # Raw index prefis output
          if "raw" in [tags] {
            mutate {
              update => { "[@metadata][index_prefix]" => "raw-%{[@metadata][index_prefix]}" }
            }
          }         
          
          
          if [TQL] and [Group] {
            # Is q TQL result - don't set an id for it - let it autogneerate

            mutate {
              add_tag => [ "healthcheck" ]
              # Write to healthcheckindiex
              update => { "[@metadata][index_prefix]" => "%{[@metadata][index_prefix]}-healthcheck-results" }
            }

          } else if "date-unknown" in [tags] {
            # Update the _id if it has a known date associated with it. OTherwisee
            mutate {
              update => { "_id" => "%{UniqueId}" }
            }
          } else {
            mutate {
              update => { "_id" => "%{UniqueId}-%{[@metadata][id_suffix]}" }
            }
          }

          # Some final cleanup
          mutate {
            remove_tag => [ "beats_input_raw_event", "ComponentFolders" ]
            # remove_field => [ "other" ]
          }

        }
      }


    03_output_tosca.conf: |
      output {

        # if "debug" in [tags] {
          stdout { codec  => rubydebug { metadata => true } }
        # } # else 
        
        if [Product] =~ "Tosca" {

          if [UniqueId] {
          
            opensearch {
              codec => json

              hosts => ["https://opensearch-cluster-master:9200"]
              index => "%{[@metadata][index_prefix]}%{[@metadata][index_suffix]}"

              user => "admin"
              password => "${OS_PASSWORD}"
              ssl => true
              ssl_certificate_verification => false
              
              # action => "create"

              document_id => "%{UniqueId}-%{[@metadata][id_suffix]}"
            }

          } else {
            opensearch {
              codec => json

              hosts => ["https://opensearch-cluster-master:9200"]
              index => "%{[@metadata][index_prefix]}%{[@metadata][index_suffix]}"

              user => "admin"
              password => "${OS_PASSWORD}"
              ssl => true
              ssl_certificate_verification => false
              
              # action => "create"
            }

          }

        } else {

          # Catch other messages to 'raw' index

          opensearch {
            # codec => json

            hosts => ["https://opensearch-cluster-master:9200"]
            index => "raw-other"

            user => "admin"
            password => "${OS_PASSWORD}"
            ssl => true
            ssl_certificate_verification => false
          }
        }
      }


  extraPorts:
    - name: test-results
      containerPort: 5044
    - name: results-http
      containerPort: 50000

  service:
    type: ClusterIP
    ports:
      - name: test-results
        port: 5044
        protocol: TCP
        targetPort: 5044
      - name: results-http
        port: 50000
        protocol: TCP
        targetPort: 50000


  # resources:
  #   requests:
  #     cpu: "100m"
  #     memory: "300Mi"
  #   limits:
  #     cpu: "500m"
  #     memory: "900Mi"


  ingress:
    enabled: false
    annotations:
      kubernetes.io/ingress.class: *ingressClassName
      nginx.ingress.kubernetes.io/whitelist-source-range: *ipwhiteliststring
    className: *ingressClassName
    pathtype: ImplementationSpecific
    hosts:
      - host: *fqdn
        paths:
          - path: /beats
            servicePort: 5044
          - path: /http
            servicePort: 50000

    tls:
     - secretName: tls-certificate

opensearch:
  install: true
  singleNode: true

  image:
    tag: "2.19.2"

  resources:
    requests:
      cpu: "250m"
      memory: "500Mi"

  startupProbe:
    initialDelaySeconds: 30

  sysctlInit:
    enabled: true

  extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    valueFrom:
      secretKeyRef:
        key:  OS_PASSWORD
        name: platform-creds
  #   - name: DISABLE_SECURITY_PLUGIN
  #     value: "false"

  # secretMounts:
  # - name: tls-certificate
  #   mountPath: /usr/share/opensearch/config
  #   path: /usr/share/opensearch/config/esnode.pem
  #   subPath: tls.crt
  # - name: tls-certificate
  #   mountPath: /usr/share/opensearch/config
  #   path: /usr/share/opensearch/config/esnode-key.pem
  #   subPath: tls.key

  config:
    # log4j2.properties: |
    #   status = error
    #
    #   appender.console.type = Console
    #   appender.console.name = console
    #   appender.console.layout.type = PatternLayout
    #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    #
    #   rootLogger.level = info
    #   rootLogger.appenderRef.console.ref = console
    opensearch.yml: |
      cluster.name: opensearch-cluster

      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0

      # discovery.type: single-node

      # TODO: Remove for prod\cust installs
      reindex.remote.whitelist: ["*.svc.cluster.local:9200", "*:920*"]
      
      reindex.ssl.verification_mode: none

      # For security plugin - session timeouts - DISABLED - these do not work
      # opensearch_security.cookie.ttl: 86400000
      # opensearch_security.session.ttl: 86400000
      # opensearch_security.session.keepalive: true

      script.painless.regex.enabled: true
      
      path.repo: ["/usr/share/opensearch/data/snapshots"]
      


  persistence:
    enabled: true
    enableInitChown: true
    labels:
      enabled: true
      additionalLabels:
        app: opensearch
    storageClass: "hostpath"
    accessModes:
      - ReadWriteOnce
    size: 1Gi

opensearch-dashboards:
  install: true
  # opensearchHosts: "https://opensearch-cluster-master:9200"
  replicaCount: 1

  image:
    tag: "2.19.1"
  # extraEnvs:
  # - name: "DISABLE_SECURITY_PLUGIN"
  #   value: "false"

  config:
     opensearch_dashboards.yml:
      server:
        name: dashboards
        host: "{{ .Values.serverHost }}"

      opensearch.ssl.verificationMode: none

      opensearch.username: "admin"
      opensearch.password: "{{ .Values.global.ospassword }}"

      assistant.chat.enabled: true

      # Dashboards TLS Config (Ensure the cert files are present before enabling SSL
      # ssl:
      #   enabled: true
      #   key: /usr/share/opensearch-dashboards/certs/dashboards-key.pem
      #   certificate: /usr/share/opensearch-dashboards/certs/dashboards-crt.pem

      # determines how dashboards will verify certificates (needs to be none for default opensearch certificates to work)
      # opensearch:
      #     certificateAuthorities: /usr/share/opensearch-dashboards/certs/dashboards-root-ca.pem
      #     if utilizing custom CA certs for connection to opensearch, provide the CA here


      # https://github.com/opensearch-project/OpenSearch-Dashboards/blob/main/config/opensearch_dashboards.yml

      # application_config.enabled: true
      # opensearchDashboards.defaultAppId: "discover" # DEPRECATED
      home.disableWelcomeScreen: true

      # Optional setting that controls the permissions of data source to create, update and delete.
      # "none": The data source is readonly for all users.
      # "dashboard_admin": The data source can only be managed by dashboard admin.
      # "all": The data source can be managed by all users. Default to "all".
      # data_source_management.manageableBy: "dashboard_admin"

      # Set the value of this setting to false to hide the help menu link to the OpenSearch Dashboards user survey
      # opensearchDashboards.survey.url: "false"

      # @experimental Set the value of this setting to display navigation updates, including dev tool top right navigation
      # opensearchDashboards.futureNavigation: false

      opensearch_security.multitenancy.enabled: true

      workspace.enabled: false
      # uiSettings:
      #   overrides:
      #     "home:useNewHomePage": true

      # Set the value to true to enable Ui Metric Collectors in Usage Collector
      # This publishes the Application Usage and UI Metrics into the saved object, which can be accessed by /api/stats?extended=true&legacy=true&exclude_usage=false
      usageCollection.uiMetric.enabled: false

      # Set the value to true to enable dynamic config service to obtain configs from a config store. By default, it's disabled
      # dynamic_config_service.enabled: false

      # Set the value to true to enable direct data import from a file
      # data_importer.enabled: true
      # Not working on 2.19.x - what version

      # assistant.chat.enabled: true

      opensearchDashboards.branding:
        logo:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        mark:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        loadingLogo:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        faviconUrl: ""
        applicationTitle: "QAI (by GroundZero)"

      # securityAnalyticsDashboards.enabled: false
      # traceAnalyticsDashboards.enabled: false
      # oberservabilityDashboards.enabled: false
      # notebooksDashboards.enabled: false
    
  ingress:
    enabled: true
    ingressClassName: *ingressClassName
    annotations:
      kubernetes.io/ingress.class: *ingressClassName
      cert-manager.io/cluster-issuer: *certificateIssuerName
      # nginx.ingress.kubernetes.io/whitelist-source-range: *ipwhiteliststring
      # nginx.ingress.kubernetes.io/use-regex: 'true'
      # nginx.ingress.kubernetes.io/rewrite-target: /$2
      # cert-manager.io/cluster-issuer: "selfsigned-issuer"
      # cert-manager.io/issuer: "selfsigned-cluster-issuer"
    hosts:
      - host: *fqdn
        paths:
          - path: /
            # pathType: ImplementationSpecific
            backend:
              serviceName: ""
              servicePort: ""
    tls:
    - hosts:
        - *fqdn
      secretName: tls-certificate
      

rabbitmq:
  install: true
  image:
    repository: bitnami/rabbitmq
  auth:
    username: admin
    password: *ospassword
    updatePassword: true
    # securePassword: true
    tls:
      enabled: false
      skipVerify: true
      verify: verify_none
      existingSecret: tls-certificate
      # existingSecretFullChain: true
      failIfNoPeerCert: false

  extraPlugins: >
    rabbitmq_auth_backend_ldap
    rabbitmq_consistent_hash_exchange
    rabbitmq_shovel
    rabbitmq_shovel_management
    rabbitmq_federation
    rabbitmq_federation_management

  serviceAccount:
    create: true

  service:
    managerPortEnabled: true

  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 150m
      memory: 250Mi

  # # Note: Not tested, should be fine though
  # persistance:
  #   enabled: false
  #   size: 1Gi
  #   storageClass: "do-block-storage-retain"
  #   accessModes:
  #     - ReadWriteOnce

  ingress:
    enabled: true
    ingressClassName: *ingressClassName
    hostname: *fqdn
    existingSecret: tls-certificate
    tls: true
    path: /ampq
    # extraPaths:
    # - path: /mqadmin
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: qarp-2-rabbitmq
    #       port:
    #         name: http-stats
    annotations:
      kubernetes.io/ingress.class: *ingressClassName
      # cert-manager.io/cluster-issuer: "letsencrypt-staging"
      nginx.ingress.kubernetes.io/whitelist-source-range: *ipwhiteliststring

kafka:
  install: false
  
  clusterId: "kafka-cluster"
  
  # image:
  #   tag: 3.9.0
  # sasl:
  #   existingSecret: message-broker-secret

  # sasl.client.users[0]: user1
  # sasl.client.passwords[0]: *msgbrokerPassword

  # # sasl.interbroker.user[0]: inter_broker_user
  # sasl.interbroker.password: *msgbrokerPassword

  # # sasl.controller.user[0]: inter_broker_user
  # sasl.controller.password: *msgbrokerPassword

  controller:
    replicaCount: 1

  provisioning:
    enabled: true
    topics:
      - name: "qa-logs"
        partitions: 1
        replicationFactor: 1
        config:
          retention.ms: 604800000 # 7 days
          segment.bytes: 1073741824 # 1GB

  resources:
    limits:
      cpu: 300m
      memory: 5Gi
    requests:
      cpu: 100m
      memory: 200Mi
  
  # TODO: If re-enable - persistance and sizing
strimzi-kafka-operator:
  install: false
  replicas: 1

kafka-ui:
  install: false

  env:
    - name: DYNAMIC_CONFIG_ENABLED
      value: "true"

  yamlApplicationConfig:
    kafka:
      clusters:
        - name: qai1-kafka
          bootstrapServers:  qai1-kafka:9092
    auth:
      type: disabled



