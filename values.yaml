
# https://github.com/elastic/helm-charts/blob/main/logstash/values.yaml
#TODO: Add opensearch output plugin!!

global:
  msgbrokerPassword: &msgbrokerPassword P@ssw0rd11!!
  ip-whitelist-intneral: &ipwhiteliststring "0.0.0.0/0"
  hostname: &hostname "g0-auth.palethorpe.biz"
  ospassword: &ospassword "Test@Password01"

logstash:
  install: true

  image: "registry.digitalocean.com/g0-cr/logstash-opensearch"
  imageTag: latest
  imagePullSecrets:
    - name: g0-cr

  persistence:
    enabled: false
  logstashConfig:
   logstash.yml: |
     http:
       host: 0.0.0.0

  envFrom:
    - secretRef:
        name: platform-creds

  logstashPipeline:
    logstash.conf: |
      # override default, hardcoded, file in template.
    01_input_default.conf: |
      input {
        beats {
          port => 5044
        }
        http {
          port => 50000
        }
      }
    01_input_file-tosca-xml.conf: |
      input {
        # file {
        # 	# tags => [ "debug" ] 

        # 	path => "/usr/share/logstash/files/export*.xml"

        # 	# TODO:
        # 	# If is an EL entry - don't take
        # 	codec => multiline {
        # 		pattern => '^\s{2}\<\w+\>\w*$'
        # 		# pattern => "<(TestCase|ExecutionList|ExecutionLogs|TestCaseLog|ActualLog)>" # AllEntries|All_TestCaseLogs|All_ExecutionLogs
        # 		negate => "true"
        # 		what => "previous"
        # 		max_lines => 20000
        # 	}

        # 	type => "xml"
        # 	start_position => "beginning"
          # }

        # http_poller {
        # 	add_field => { 
        # 		"product" => "healthcheck"
        # 		"ccode" => "csr"
        # 		"country_code" => "au"
        # 	}
        # 	urls => {
        # 		consul_health_state => "http://${CONSUL_HEALTH_URL:localhost}:8500/v1/health/state/any"
        # 	}

        # 	codec => "json"
        # 	schedule => { every => "60s" }
        # }
      }
    02_filter_tosca.conf: |
      filter {
        if [Product] == "Tosca" and ![AddedToListDate] {

          # Get rid of the header message in xml
          if [message] =~ /xml version=/ { drop { } }


          # xml {
          # 	source => "message"
          # 	store_xml => true
          # 	target => "_xml"
          # 	force_array => false
          # 	# max_lines => 5000
          # }

          mutate {
            add_field => { "[@metadata][index_prefix]" => "%{Product}-%{CustCode}" }
            lowercase => [ "[@metadata][index_prefix]" ]
            # add_field => { "[index_prefix]" => "%{[@metadata][index_prefix]}" }
            add_field => { "eventtime"=> "%{@timestamp}" }
          }

          ruby {
            code => '
              x = event.get("[@metadata][index_prefix]").downcase
              event.set("[@metadata][index_prefix]",x)

              y = event.get("_xml")

              if !y.nil?
                y.each {|k, v|
                  v.gsub!(/^true$/i,"true") if !v.nil?
                  v.gsub!(/^false$/i,"false") if !v.nil?
                  event.set(k,v)
                }
              end

              # Keep metadata
              event.get("[@metadata]").each {|k,v| event.set("[meta][#{k}]", v)}

              event.remove("_xml")
              event.remove("message")
            '
          }

          # Workaround for value of 'unknown'
          if [CreatedAt] == "Unknown" {
            mutate {
              update => { "CreatedAt" => "%{ModifiedAt}" }
            }
          }

          date {
            match => ["[ModifiedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "@timestamp"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            match => ["[ModifiedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "ModifiedAt"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            match => ["[ModifiedAtDate]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "ModifiedAtDate"
            tag_on_failure => ["_dateparsefailure_ModifiedAtDate"]
          }

          date {
            match => ["[CreatedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "CreatedAt"
            tag_on_failure => ["_dateparsefailure_CreatedAt"]
          }

          date {
            match => ["[CreatedAtDate]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "CreatedAtDate"
            tag_on_failure => ["_dateparsefailure_CreatedAtDate"]
          }

          # Set Execution list evetime time to StartTime
          date {
            match => ["[StartTime]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "@timestamp"
            tag_on_failure => ["_dateparsefailure_StartTime"]
          }

          # Set the StartTime time to be the eventime
          if [ObjectType] == "ExecutionTestCaseLog" and [StartTime] {
            mutate {
              update => { "StartTime" => "%{@timestamp}" }
            }
          }

          date {
            match => ["[EndTime]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "EndTime"
            tag_on_failure => ["_dateparsefailure_EndTime"]
          }


          mutate {
            gsub => [ "NodePath", "^\s*", "", "message", "\s*$", ""]

            convert => { "UpdateRevision" => "integer" }
            convert => { "Revision" => "integer" }

            convert => { "HasMissingReferences" => "boolean" }
            convert => { "IsTemplate" => "boolean" }
            convert => { "Disabled" => "boolean" }
            convert => { "IncludeForSynchronization" => "boolean" }
            convert => { "IsBusinessTestCase" => "boolean" }
            convert => { "ChangesAllowed" => "boolean" }
            convert => { "IsCheckedOutByMe" => "boolean" }

            # ExecutionList
            convert => { "NumberOfTestCases" => "integer" }
            convert => { "NumberOfTestCasesFailed" => "integer" }
            convert => { "NumberOfTestCasesPassed" => "integer" }
            convert => { "NumberOfTestCasesNotExecuted" => "integer" }
            convert => { "NumberOfTestCasesWithUnknownState" => "integer" }

            convert => { "IsViewingAllowed" => "boolean" }
            convert => { "IsOsvItem" => "boolean" }
            convert => { "HasNotUniqueNamedSubparts" => "boolean" }
            convert => { "OwningGroupInherited" => "boolean" }
            convert => { "IncludeInAnalytics" => "boolean" }
            convert => { "IsMandate" => "boolean" }
            convert => { "HasMandates" => "boolean" }
            convert => { "ContainsClassic" => "boolean" }
            convert => { "IsBusinessExecutionList" => "boolean" }
            convert => { "HasInconsistentParallelizationState" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }

            convert => { "ManualItems" => "integer" }
            convert => { "AutomatedItems" => "integer" }

            #TestCaseLog
            convert => { "IsDeletedFromFileService" => "boolean" }
            convert => { "RetainOnFileService" => "boolean" }
            convert => { "Canceled" => "boolean" }
            convert => { "Recovered" => "boolean" }

            convert => { "Duration" => "float" }

            # TODO: Only remove original event if NOT failed processing
            remove_field => [ "[event][original]", "[$id]" ]
            # remove_field => [ "[log][file]][path]" ]
          }

          # Drop soem duplicates
          # if [ObjectType] == 'TestCase' and [UpdateRevision] == ''  { drop { } }

          kv {
            source => "TCProperties"
            target => "[Properties]"
            include_brackets => false
            field_split => ' '
            trim_key => " "
            remove_field => [ "TCProperties" ]
          }

          ruby  {
            code => '
              
              # begin 

              r = event.get("[NodePath]")
              if (r.to_s.scan("/").count > 0 && r !=nil )
                paths = r.split("/").reject {|s| s.empty? }

                t = paths.delete_if { |component| component.empty? }
                event.set("[ComponentFolders]", t) 

              end


              # rescue
              # end
            '}

          ruby {
            code => '
              t = Time.at(event.get("@timestamp").to_f)
              suffix = t.strftime("%Y%m%d%H%M%S")
              event.set("[@metadata][id_suffix]", suffix)

              suffix = t.strftime("%Y-%m")
              event.set("[@metadata][index_suffix]", "-" + suffix)

            '
          }

          mutate {
            update => { "_id" => "%{UniqueId}-%{[@metadata][id_suffix]}" }
          }

        }
      }


    03_output_tosca.conf: |
      output {

        if "debug" in [tags] {
          stdout { codec  => rubydebug { metadata => true } }
        } else {
          # elasticsearch {
          # 	hosts => "host.docker.internal:9200"
          # 	codec => json
          # 	index => "%{[@metadata][index_prefix]}"	# TODO: Add date (month\year) suffix to filter
          # 	# user => "elastic" # sort out auth!!
          # 	# password => "${LOGSTASH_INTERNAL_PASSWORD}"
        
          # 	# Specific for Tricentis - as setting the docuemnt id based on the UUID + revisison
          # 	document_id => "%{UniqueId}-%{[@metadata][index_suffix]}"
          # }

          opensearch {
            codec => json

            hosts => ["https://opensearch-cluster-master:9200"]
            index => "%{[@metadata][index_prefix]}%{[@metadata][index_suffix]}"

            user => "admin"
            password => "Test@Password01"
            ssl => true
            ssl_certificate_verification => false
            
            # action => "create"

            document_id => "%{UniqueId}-%{[@metadata][id_suffix]}"
          }

        }
      }

  extraPorts:
    - name: test-results
      containerPort: 5044
    - name: results-http
      containerPort: 50000


  # resources:
  #   requests:
  #     cpu: "100m"
  #     memory: "300Mi"
  #   limits:
  #     cpu: "500m"
  #     memory: "900Mi"


  ingress:
    enabled: false
    annotations:
      kubernetes.io/ingress.class: "nginx"
      # cert-manager.io/cluster-issuer: "letsencrypt-prod"
    className: "nginx"
    pathtype: ImplementationSpecific
    hosts:
      - host: *hostname
        paths:
          - path: /ls
            servicePort: 5044
    tls:
     - secretName: g0-auth


  service:
    type: ClusterIP
    ports:
      - name: test-results
        port: 5044
        protocol: TCP
        targetPort: 5044
      - name: results-http
        port: 5050
        protocol: TCP
        targetPort: 50000

opensearch:
  install: true
  singleNode: true

  image:
    tag: "2.19.1"

  resources:
    requests:
      cpu: "150m"
      memory: "100Mi"

  startupProbe:
    initialDelaySeconds: 30

  sysctlInit:
    enabled: true

  extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    valueFrom:
      secretKeyRef:
        key:  opensearch-admin-password
        name: platform-creds
  #   - name: DISABLE_SECURITY_PLUGIN
  #     value: "false"
  config:
    # log4j2.properties: |
    #   status = error
    #
    #   appender.console.type = Console
    #   appender.console.name = console
    #   appender.console.layout.type = PatternLayout
    #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    #
    #   rootLogger.level = info
    #   rootLogger.appenderRef.console.ref = console
    opensearch.yml: |
      cluster.name: opensearch-cluster

      # Bind to all interfaces because we don't know what IP address Docker will assign to us.
      network.host: 0.0.0.0

      # discovery.type: single-node

      script.painless.regex.enabled: true

  # plugins:
  #   enabled: false
  #   installList:
  #   - example-fake-plugin

  persistence:
    enabled: true
    enableInitChown: true
    labels:
      enabled: true
      additionalLabels:
        app: opensearch
        customer-code: g001 
    storageClass: "do-block-storage-retain"
    accessModes:
      - ReadWriteOnce
    size: 1Gi

opensearch-dashboards:
  install: true
  # opensearchHosts: "https://opensearch-cluster-master:9200"
  replicaCount: 1

  image:
    tag: "2.19.1"
  # extraEnvs:
  # - name: "DISABLE_SECURITY_PLUGIN"
  #   value: "false"

  config:
     opensearch_dashboards.yml:
      server:
        name: dashboards
        host: "{{ .Values.serverHost }}"

      opensearch.ssl.verificationMode: none

      opensearch.username: "admin"
      opensearch.password: "{{ .Values.global.ospassword }}"

      # Dashboards TLS Config (Ensure the cert files are present before enabling SSL
      # ssl:
      #   enabled: true
      #   key: /usr/share/opensearch-dashboards/certs/dashboards-key.pem
      #   certificate: /usr/share/opensearch-dashboards/certs/dashboards-crt.pem

      # determines how dashboards will verify certificates (needs to be none for default opensearch certificates to work)
      # opensearch:
      #     certificateAuthorities: /usr/share/opensearch-dashboards/certs/dashboards-root-ca.pem
      #     if utilizing custom CA certs for connection to opensearch, provide the CA here


      # https://github.com/opensearch-project/OpenSearch-Dashboards/blob/main/config/opensearch_dashboards.yml

      # application_config.enabled: true
      # opensearchDashboards.defaultAppId: "discover" # DEPRECATED
      home.disableWelcomeScreen: true

      # Optional setting that controls the permissions of data source to create, update and delete.
      # "none": The data source is readonly for all users.
      # "dashboard_admin": The data source can only be managed by dashboard admin.
      # "all": The data source can be managed by all users. Default to "all".
      # data_source_management.manageableBy: "dashboard_admin"

      # Set the value of this setting to false to hide the help menu link to the OpenSearch Dashboards user survey
      # opensearchDashboards.survey.url: "false"

      # @experimental Set the value of this setting to display navigation updates, including dev tool top right navigation
      # opensearchDashboards.futureNavigation: false

      opensearch_security.multitenancy.enabled: true

      workspace.enabled: false
      # uiSettings:
      #   overrides:
      #     "home:useNewHomePage": true

      # Set the value to true to enable Ui Metric Collectors in Usage Collector
      # This publishes the Application Usage and UI Metrics into the saved object, which can be accessed by /api/stats?extended=true&legacy=true&exclude_usage=false
      usageCollection.uiMetric.enabled: false

      # Set the value to true to enable dynamic config service to obtain configs from a config store. By default, it's disabled
      # dynamic_config_service.enabled: false

      # Set the value to true to enable direct data import from a file
      # data_importer.enabled: true
      # Not working on 2.19.x - what version

      # assistant.chat.enabled: true

      opensearchDashboards.branding:
        logo:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        mark:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        loadingLogo:
          defaultUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
          darkModeUrl: "https://dev.w3.org/SVG/tools/svgweb/samples/svg-files/atom.svg"
        faviconUrl: ""
        applicationTitle: "QAI (by GroundZero)"

      # securityAnalyticsDashboards.enabled: false
      # traceAnalyticsDashboards.enabled: false
      # oberservabilityDashboards.enabled: false
      # notebooksDashboards.enabled: false
    
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      kubernetes.io/ingress.class: "nginx"
      nginx.ingress.kubernetes.io/whitelist-source-range: *ipwhiteliststring
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
    hosts:
      - host: *hostname
        paths:
          - path: /
            backend:
              serviceName: ""
              servicePort: ""
    tls:
    - secretName: g0-auth
      hosts:
        - *hostname

rabbitmq:
  install: false
  image:
    repository: bitnami/rabbitmq
  auth:
    # username: admin
    password: rabbitmqpassword
    updatePassword: true
    # securePassword: true
    tls:
      enabled: false
      skipVerify: true
      verify: verify_none
      # existingSecret: g0-auth
      # existingSecretFullChain: true
      failIfNoPeerCert: false

  extraPlugins: >
    rabbitmq_auth_backend_ldap
    rabbitmq_consistent_hash_exchange
    rabbitmq_shovel
    rabbitmq_shovel_management
    rabbitmq_federation
    rabbitmq_federation_management

  serviceAccount:
    create: true

  service:
    managerPortEnabled: true

  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 150m
      memory: 250Mi



  ingress:
    enabled: true
    ingressClassName: nginx
    hostname: *hostname
    existingSecret: g0-auth
    tls: true
    path: /ampq
    # extraPaths:
    # - path: /mqadmin
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: qarp-2-rabbitmq
    #       port:
    #         name: http-stats
    annotations:
      kubernetes.io/ingress.class: "nginx"
      # cert-manager.io/cluster-issuer: "letsencrypt-staging"
      nginx.ingress.kubernetes.io/whitelist-source-range: *ipwhiteliststring

kafka:
  install: false
  
  clusterId: "kafka-cluster"
  # replicaCount: 1
  
  # image:
  #   tag: 3.9.0

  # # extraEnvVars:
  # #   - name: KAFKA_CFG_CONTROLLER_QUORUM_VOTERS
  # #     value: "1@127.0.0.1:9094"

  # sasl:
  #   existingSecret: message-broker-secret

  # sasl.client.users[0]: user1
  # sasl.client.passwords[0]: *msgbrokerPassword

  # # sasl.interbroker.user[0]: inter_broker_user
  # sasl.interbroker.password: *msgbrokerPassword

  # # sasl.controller.user[0]: inter_broker_user
  # sasl.controller.password: *msgbrokerPassword

  # controller:
  #   replicaCount: 1
    # quorumBootstrapServers: "127.0.0.1:9094"
    # args:
    #   - --standalone

  resources:
    limits:
      cpu: 300m
      memory: 5Gi
    requests:
      cpu: 100m
      memory: 200Mi

kafka-ui:
  install: false

  env:
    - name: DYNAMIC_CONFIG_ENABLED
      value: "true"

  # yamlApplicationConfig:
  #   kafka:
  #     clusters:
  #       - name: kafka-cluster
  #         bootstrapServers:  kafka-cluster:9092
  #   auth:
  #     type: disabled
  #   management:
  #     health:
  #       ldap:
  #         enabled: false



