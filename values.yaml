
# https://github.com/elastic/helm-charts/blob/main/logstash/values.yaml
#TODO: Add opensearch output plugin!!
logstash:
  install: true

  image: "registry.digitalocean.com/g0-cr/logstash-opensearch"
  imageTag: "latest"
  imagePullSecrets:
    - name: g0-cr

  persistence:
    enabled: false
  logstashConfig:
   logstash.yml: |
     http:
       host: 0.0.0.0
  logstashPipeline:
    logstash.conf: |
      # override default, hardcoded, file in template.
    01_input_default.conf: |
      input {
        # beats {
        #   port => 5044
        # }

        tcp {
        	port => 50000
          codec => "json"
        	add_field => {
        		"Product" => "Tosca"
        		"CustCode" => "CSR"
        		"Country" => "AU"
        	}
        }
      }

    01_input_file-tosca-xml.conf: |
      input {
        # file {
        # 	# tags => [ "debug" ] 

        # 	path => "/usr/share/logstash/files/export*.xml"

        # 	# TODO:
        # 	# If is an EL entry - don't take
        # 	codec => multiline {
        # 		pattern => '^\s{2}\<\w+\>\w*$'
        # 		# pattern => "<(TestCase|ExecutionList|ExecutionLogs|TestCaseLog|ActualLog)>" # AllEntries|All_TestCaseLogs|All_ExecutionLogs
        # 		negate => "true"
        # 		what => "previous"
        # 		max_lines => 20000
        # 	}

        # 	type => "xml"
        # 	start_position => "beginning"
          # }

        # http_poller {
        # 	add_field => { 
        # 		"product" => "healthcheck"
        # 		"ccode" => "csr"
        # 		"country_code" => "au"
        # 	}
        # 	urls => {
        # 		consul_health_state => "http://${CONSUL_HEALTH_URL:localhost}:8500/v1/health/state/any"
        # 	}

        # 	codec => "json"
        # 	schedule => { every => "60s" }
        # }
      }
    02_filter_tosca.conf: |
      filter {
        if [Product] == "Tosca" and ![AddedToListDate] {

          # Get rid of the header message in xml
          if [message] =~ /xml version=/ { drop { } }


          # xml {
          # 	source => "message"
          # 	store_xml => true
          # 	target => "_xml"
          # 	force_array => false
          # 	# max_lines => 5000
          # }

          mutate {
            add_field => { "[@metadata][index_prefix]" => "%{Product}-%{CustCode}" }
            lowercase => [ "[@metadata][index_prefix]" ]
            # add_field => { "[index_prefix]" => "%{[@metadata][index_prefix]}" }
          }

          ruby {
            code => '
              x = event.get("[@metadata][index_prefix]").downcase
              event.set("[@metadata][index_prefix]",x)

              y = event.get("_xml")

              if !y.nil?
                y.each {|k, v|
                  v.gsub!(/^true$/i,"true") if !v.nil?
                  v.gsub!(/^false$/i,"false") if !v.nil?
                  event.set(k,v)
                }
              end

              event.remove("_xml")
              event.remove("message")
            '
          }

          # Workaround for value of 'unknown'
          if [CreatedAt] == "Unknown" {
            mutate {
              update => { "CreatedAt" => "%{ModifiedAt}" }
            }
          }

          date {
            match => ["[ModifiedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "@timestamp"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            match => ["[ModifiedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "ModifiedAt"
            # remove_field => [ "ModifiedAt" ]
            tag_on_failure => ["_dateparsefailure_ModifiedAt"]
          }

          date {
            match => ["[ModifiedAtDate]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "ModifiedAtDate"
            tag_on_failure => ["_dateparsefailure_ModifiedAtDate"]
          }

          date {
            match => ["[CreatedAt]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "CreatedAt"
            tag_on_failure => ["_dateparsefailure_CreatedAt"]
          }

          date {
            match => ["[CreatedAtDate]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "CreatedAtDate"
            tag_on_failure => ["_dateparsefailure_CreatedAtDate"]
          }

          # Set Execution list evetime time to StartTime
          date {
            match => ["[StartTime]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "StartTime"
            tag_on_failure => ["_dateparsefailure_StartTime"]
          }

          date {
            match => ["[EndTime]","dd/MM/yyyy h:mm:ss a","dd/M/yyyy h:mm:ss a"] # "MM/dd/yyyy h:mm:ss a","M/dd/yyyy h:mm:ss a",
            # timezone => "Australia/Sydney"
            target => "EndTime"
            tag_on_failure => ["_dateparsefailure_EndTime"]
          }


          mutate {
            gsub => [ "NodePath", "^\s*", "", "message", "\s*$", ""]

            convert => { "UpdateRevision" => "integer" }
            convert => { "Revision" => "integer" }

            convert => { "HasMissingReferences" => "boolean" }
            convert => { "IsTemplate" => "boolean" }
            convert => { "Disabled" => "boolean" }
            convert => { "IncludeForSynchronization" => "boolean" }
            convert => { "IsBusinessTestCase" => "boolean" }
            convert => { "ChangesAllowed" => "boolean" }
            convert => { "IsCheckedOutByMe" => "boolean" }

            # ExecutionList
            convert => { "NumberOfTestCases" => "integer" }
            convert => { "NumberOfTestCasesFailed" => "integer" }
            convert => { "NumberOfTestCasesPassed" => "integer" }
            convert => { "NumberOfTestCasesNotExecuted" => "integer" }
            convert => { "NumberOfTestCasesWithUnknownState" => "integer" }

            convert => { "IsViewingAllowed" => "boolean" }
            convert => { "IsOsvItem" => "boolean" }
            convert => { "HasNotUniqueNamedSubparts" => "boolean" }
            convert => { "OwningGroupInherited" => "boolean" }
            convert => { "IncludeInAnalytics" => "boolean" }
            convert => { "IsMandate" => "boolean" }
            convert => { "HasMandates" => "boolean" }
            convert => { "ContainsClassic" => "boolean" }
            convert => { "IsBusinessExecutionList" => "boolean" }
            convert => { "HasInconsistentParallelizationState" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }
            convert => { "IncludeForAccumulation" => "boolean" }

            convert => { "ManualItems" => "integer" }
            convert => { "AutomatedItems" => "integer" }

            #TestCaseLog
            convert => { "IsDeletedFromFileService" => "boolean" }
            convert => { "RetainOnFileService" => "boolean" }
            convert => { "Canceled" => "boolean" }
            convert => { "Recovered" => "boolean" }

            convert => { "Duration" => "float" }

            # TODO: Only remove original event if NOT failed processing
            remove_field => [ "[event][original]", "[$id]" ]
            # remove_field => [ "[log][file]][path]" ]
          }

          # Drop soem duplicates
          # if [ObjectType] == 'TestCase' and [UpdateRevision] == ''  { drop { } }

          kv {
            source => "TCProperties"
            target => "[Properties]"
            include_brackets => false
            field_split => ' '
            trim_key => " "
            remove_field => [ "TCProperties" ]
          }

          ruby  {
            code => '
              
              # begin 

              r = event.get("[NodePath]")
              if (r.to_s.scan("/").count > 0 && r !=nil )
                paths = r.split("/").reject {|s| s.empty? }

                t = paths.delete_if { |component| component.empty? }
                event.set("[ComponentFolders]", t) 

              end

              # rescue
              # end
            '}

          ruby {
            code => '
              t = Time.at(event.get("@timestamp").to_f)
              suffix = t.strftime("%Y%d%H%M%S")
              event.set("[@metadata][id_suffix]", suffix)

              suffix = t.strftime("%Y")
              event.set("[@metadata][index_suffix]", "-" + suffix)

            '
          }

          mutate {
            update => { "_id" => "%{UniqueId}-%{[@metadata][id_suffix]}" }
          }

        }
      }

    03_output_tosca.conf: |
      output {

        if "debug" in [tags] {
          stdout { codec  => rubydebug { metadata => true } }
        } else {
          # elasticsearch {
          # 	hosts => "host.docker.internal:9200"
          # 	codec => json
          # 	index => "%{[@metadata][index_prefix]}"	# TODO: Add date (month\year) suffix to filter
          # 	# user => "elastic" # sort out auth!!
          # 	# password => "${LOGSTASH_INTERNAL_PASSWORD}"
        
          # 	# Specific for Tricentis - as setting the docuemnt id based on the UUID + revisison
          # 	document_id => "%{UniqueId}-%{[@metadata][index_suffix]}"
          # }

          opensearch {
            codec => json

            hosts => ["https://opensearch-cluster-master:9200"]
            index => "%{[@metadata][index_prefix]}%{[@metadata][index_suffix]}"

            user => "admin"
            password => "Test@Password01"
            ssl => true
            ssl_certificate_verification => false
            
            # action => "create"

            document_id => "%{UniqueId}-%{[@metadata][id_suffix]}"
          }

        }
      }


  extraPorts:
    - name: test-results
      containerPort: 50000

  service:
    # annotations: {}
    type: ClusterIP
    # loadBalancerIP: ""
    ports:
      - name: test-results
        port: 50000
        protocol: TCP
        targetPort: 50000

opensearch:
  install: true
  singleNode: true
  resources:
    requests:
      cpu: "150m"
      memory: "100Mi"

  startupProbe:
    initialDelaySeconds: 30

  sysctlInit:
    enabled: true

  extraEnvs:
  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD
    valueFrom:
      secretKeyRef:
        key:  opensearch-admin-password
        name: platform-creds
  #   - name: DISABLE_SECURITY_PLUGIN
  #     value: "false"
  config:
    # log4j2.properties: |
    #   status = error
    #
    #   appender.console.type = Console
    #   appender.console.name = console
    #   appender.console.layout.type = PatternLayout
    #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    #
    #   rootLogger.level = info
    #   rootLogger.appenderRef.console.ref = console

  # plugins:
  #   enabled: false
  #   installList:
  #   - example-fake-plugin

  persistence:
    enabled: true
    enableInitChown: true
    labels:
      enabled: true
      additionalLabels:
        app: opensearch
        customer-code: g001 
    storageClass: "do-block-storage-retain"
    accessModes:
      - ReadWriteOnce
    size: 1Gi

opensearch-dashboards:
  opensearchHosts: "https://opensearch-cluster-master:9200"
  # image:
  #   tag: "2.18.0"
  # extraEnvs:
  # - name: "DISABLE_SECURITY_PLUGIN"
  #   value: "false"

  config:
     opensearch_dashboards.yml:
      server:
        name: dashboards
        host: "{{ .Values.serverHost }}"

      opensearch.ssl.verificationMode: none

      opensearch.username: "admin"
      opensearch.password: "Test@Password01"

      # Dashboards TLS Config (Ensure the cert files are present before enabling SSL
      # ssl:
      #   enabled: true
      #   key: /usr/share/opensearch-dashboards/certs/dashboards-key.pem
      #   certificate: /usr/share/opensearch-dashboards/certs/dashboards-crt.pem

      # determines how dashboards will verify certificates (needs to be none for default opensearch certificates to work)
      # opensearch:
      #     certificateAuthorities: /usr/share/opensearch-dashboards/certs/dashboards-root-ca.pem
      #     if utilizing custom CA certs for connection to opensearch, provide the CA here
    
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      kubernetes.io/ingress.class: "nginx"
      # nginx.ingress.kubernetes.io/whitelist-source-range: "163.47.122.20,159.196.170.110"
      cert-manager.io/cluster-issuer: "letsencrypt-prod"
    hosts:
      - host: g0-auth.palethorpe.biz
        paths:
          - path: /
            backend:
              serviceName: ""
              servicePort: ""
    tls:
    - secretName: g0-auth
      hosts:
        - g0-auth.palethorpe.biz

rabbitmq:
  install: true
  image:
    repository: bitnami/rabbitmq
  auth:
    # username: admin
    password: rabbitmqpassword
    updatePassword: true
    # securePassword: true
    tls:
      enabled: false
      skipVerify: true
      verify: verify_none
      # existingSecret: default-tls-cert
      # existingSecretFullChain: true
      failIfNoPeerCert: false

  extraPlugins: >
    rabbitmq_auth_backend_ldap
    rabbitmq_consistent_hash_exchange
    rabbitmq_shovel
    rabbitmq_shovel_management
    rabbitmq_federation
    rabbitmq_federation_management

  serviceAccount:
    create: true

  service:
    managerPortEnabled: false

  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 150m
      memory: 250Mi



  ingress:
    enabled: false
    ingressClassName: nginx
    hostname: g0-auth-os.palethorpe.biz
    tls: true
    # path: /*
    # extraPaths:
    #   - path: /mq/*
    #     pathType: ImplementationSpecific
    #     backendName: 
    #       serviceName: qarp-2-rabbitmq
    #       servicePort: amqp
    annotations:
      kubernetes.io/ingress.class: "nginx"
      cert-manager.io/cluster-issuer: "letsencrypt-staging"
